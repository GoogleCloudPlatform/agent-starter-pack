# Copyright 2026 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ruff: noqa

from google_cloud_pipeline_components.types.artifact_types import BQTable
from kfp.dsl import Input, component


@component(
    base_image="us-docker.pkg.dev/production-ai-template/starter-pack/data_processing_vectorsearch:0.3",
)
def ingest_data(
    project_id: str,
    location: str,
    collection_id: str,
    ingestion_batch_size: int,
    input_table: Input[BQTable],
) -> None:
    """Ingest processed data into Vector Search 2.0 Collection.

    Reads chunks from BigQuery and batch-creates data objects in the
    VS 2.0 Collection. Embeddings are auto-generated by the Collection's
    configured embedding model.

    Args:
        project_id: Google Cloud project ID
        location: Vector Search location
        collection_id: Vector Search 2.0 Collection ID
        ingestion_batch_size: Number of data objects per batch request
        input_table: Input BQ table with processed chunks
    """
    import logging

    import bigframes.pandas as bpd
    from google.cloud import vectorsearch_v1beta

    # Initialize logging
    logging.basicConfig(level=logging.INFO)

    # Initialize clients
    logging.info("Initializing clients...")
    bpd.options.bigquery.project = project_id
    bpd.options.bigquery.location = location

    data_object_client = vectorsearch_v1beta.DataObjectServiceClient()
    collection_path = (
        f"projects/{project_id}/locations/{location}/collections/{collection_id}"
    )
    logging.info("Clients initialized.")

    dataset = input_table.metadata["datasetId"]
    table = input_table.metadata["tableId"]

    query = f"""
        SELECT
            question_id
            , full_text_md
            , text_chunk
            , chunk_id
        FROM {project_id}.{dataset}.{table}
    """
    df = bpd.read_gbq(query).to_pandas()
    logging.info(f"Read {len(df)} rows from BigQuery.")

    # Batch create data objects in Vector Search 2.0
    # Max 250 per request for auto-embeddings
    batch_size = min(ingestion_batch_size, 250)
    for batch_start in range(0, len(df), batch_size):
        batch_end = min(batch_start + batch_size, len(df))
        batch_df = df.iloc[batch_start:batch_end]

        batch_request = [
            {
                "data_object_id": str(row["chunk_id"]),
                "data_object": {
                    "data": {
                        "question_id": str(row["question_id"]),
                        "text_chunk": str(row["text_chunk"]),
                        "full_text_md": str(row["full_text_md"]),
                    },
                    "vectors": {},  # Empty vectors â€” auto-generated by VS 2.0
                },
            }
            for _, row in batch_df.iterrows()
        ]

        try:
            request = vectorsearch_v1beta.BatchCreateDataObjectsRequest(
                parent=collection_path,
                requests=batch_request,
            )
            data_object_client.batch_create_data_objects(request)
        except Exception as e:
            if "already exists" not in str(e).lower():
                logging.warning(
                    f"Batch {batch_start // batch_size + 1} error: {str(e)[:200]}"
                )

        if (batch_start // batch_size + 1) % 10 == 0:
            logging.info(f"Processed {batch_end}/{len(df)} data objects...")

    logging.info(f"Ingestion complete. {len(df)} data objects created.")
