{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating CrewAI Agent\n",
    "\n",
    "This notebook demonstrates how to test and evaluate the CrewAI agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Prerequisites\n\n- Google Cloud project with Vertex AI enabled\n- Authenticated with `gcloud auth application-default login`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport google.auth\nimport sys\n\n# Authenticate\ncredentials, project_id = google.auth.default()\nprint(f\"Project ID: {project_id}\")\n\n# Set environment variables\nos.environ[\"GOOGLE_CLOUD_PROJECT\"] = project_id\nprint(\"âœ“ Vertex AI authentication configured\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "sys.path.append('../')\n\nfrom {{cookiecutter.agent_directory}}.agent import (\n    run_agent,\n    create_crew,\n    assistant_agent,\n    calculate,\n    analyze_text,\n    get_current_time,\n    generate_ideas\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Test Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define test queries for different tool types\ntest_queries = [\n    \"What time is it?\",\n    \"Calculate 15 * 23 + 47\",\n    \"Analyze this text: 'CrewAI makes building AI agents simple and powerful!'\",\n    \"Generate 3 ideas for improving productivity at work\",\n]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Test Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Run queries and collect results\n",
    "results = []\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        response = run_agent(query)\n",
    "        print(f\"Response: {response}\\n\")\n",
    "        \n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            \"response_length\": len(response),\n",
    "            \"success\": True\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\\n\")\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"response\": f\"Error: {e}\",\n",
    "            \"response_length\": 0,\n",
    "            \"success\": False\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nResults Summary:\")\n",
    "print(f\"Total queries: {len(df)}\")\n",
    "print(f\"Successful: {df['success'].sum()}\")\n",
    "print(f\"Failed: {(~df['success']).sum()}\")\n",
    "print(f\"\\nAverage response length: {df['response_length'].mean():.0f} characters\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Inspect agent configuration\nprint(\"Agent Configuration:\")\nprint(f\"Role: {assistant_agent.role}\")\nprint(f\"Goal: {assistant_agent.goal}\")\nprint(f\"Number of tools: {len(assistant_agent.tools)}\")\nprint(f\"Tools: {[tool.name for tool in assistant_agent.tools]}\")\nprint(f\"Allow delegation: {assistant_agent.allow_delegation}\")\nprint(f\"Verbose: {assistant_agent.verbose}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Testing\n",
    "\n",
    "Measure response time for queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Test query performance\n",
    "test_query = \"What time is it?\"\n",
    "\n",
    "print(f\"Testing query: {test_query}\")\n",
    "start_time = time.time()\n",
    "response = run_agent(test_query)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nResponse time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Insights\n",
    "\n",
    "### Key Observations:\n",
    "\n",
    "1. **Response Quality**: Evaluate if responses are accurate and helpful\n",
    "2. **Response Time**: Check if performance is acceptable\n",
    "3. **Tool Usage**: Verify agent uses appropriate tools for each query\n",
    "4. **Error Handling**: Assess how agent handles edge cases\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Fine-tune agent parameters (temperature, model selection)\n",
    "- Add more specialized tools\n",
    "- Implement caching for frequently asked questions\n",
    "- Deploy to production environment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}